{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b46038-90e5-44b5-b31a-275f9b391081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, lit, col, count, avg, round, explode\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "\n",
    "\n",
    "spark_conf = SparkConf()\n",
    "spark_conf.set(\"spark.executor.memory\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"4g\")\n",
    "spark_conf.set(\"spark.network.timeout\", \"600s\")\n",
    "spark_conf.set(\"spark.executor.instances\", \"4\")\n",
    "spark_conf.set(\"spark.executor.cores\", \"4\")\n",
    "spark_conf.set(\"spark.default.parallelism\", \"6\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"6\")\n",
    "spark_conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.enableHiveSupport().config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    ".config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\") \\\n",
    ".config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    ".config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    ".config(conf=spark_conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2355080-437e-41c6-a892-b9e66a4440cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_layer = [\n",
    "    [\"enem_microdados\", \"raw\"],\n",
    "    [\"dim_candidato\", \"trusted\"],\n",
    "    [\"dim_escola\", \"trusted\"],\n",
    "    [\"dim_status_redacao\", \"trusted\"],\n",
    "    [\"dim_tipo_prova\", \"trusted\"],\n",
    "    [\"fact_candidato\", \"trusted\"],\n",
    "    [\"fact_candidato_denormalized\", \"analytics\"]\n",
    "]\n",
    "\n",
    "df_grouped = None\n",
    "for table_info in table_layer:\n",
    "    table = table_info[0]\n",
    "    layer = table_info[1]\n",
    "    print(f\"{table} - {layer}\")\n",
    "\n",
    "    df = spark.read.format(\"delta\").load(\n",
    "        (\n",
    "            os.path.join(\n",
    "                os.getcwd(), \"layers\", layer, \"tables\", table\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    faulty_records = 0\n",
    "    try:\n",
    "        df_error = spark.read.format(\"delta\").load(\n",
    "            (\n",
    "                os.path.join(\n",
    "                    os.getcwd(), \"layers\", layer, \"tables\", f\"{table}_errors\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        faulty_records = df_error.count()\n",
    "        \n",
    "        def extrair_valor(error_list):\n",
    "            valor = list(list(error_list.items())[0])\n",
    "            return valor\n",
    "        \n",
    "        schema_valor = ArrayType(StringType())\n",
    "        udf_extrair_valor = udf(extrair_valor, schema_valor)\n",
    "\n",
    "        df_exploded = df_error.select(explode(\"error_list\").alias(\"explodido\"))\n",
    "        df_exploded = df_exploded.select(explode(\"explodido\").alias(\"explodido\"))\n",
    "        df_result = df_exploded.withColumn(\"_explodido\", udf_extrair_valor(\"explodido\"))\n",
    "        df_result = df_result.withColumn(\"valor\", col(\"_explodido\").getItem(1))\n",
    "        df_result = df_result.drop(\"_chave\",\"explodido\",\"_explodido\").drop_duplicates()\n",
    "        faulty_dqs = \",\".join([value[0] for value in df_result.collect()])\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        df_error = None\n",
    "        faulty_records = 0\n",
    "        faulty_dqs = None\n",
    "        has_error = False\n",
    "\n",
    "    df_columns = df.columns  \n",
    "    sys_columns = ['ingestion_at','YEAR','MONTH','DAY']\n",
    "    columns = list(filter(lambda x: x not in sys_columns, df_columns))\n",
    "\n",
    "    valid_records = df.count()\n",
    "    total_records = valid_records+faulty_records\n",
    "    \n",
    "    if df_error:\n",
    "        df_table_grouped = (\n",
    "            df_error.groupBy().agg(avg(\"total_quality_errors\").alias(\"avg_total_quality_errors\"))\n",
    "                        .withColumn(\"total_faulty_columns_perc\", (col(\"avg_total_quality_errors\") / lit(len(columns)))*100) \n",
    "                        .withColumn(\"table_name\", lit(table)) \n",
    "                        .withColumn(\"layer\", lit(layer))\n",
    "                        .withColumn(\"total_columns_num\", lit(len(columns)))\n",
    "                        .withColumn(\"total_records\", lit(total_records)) \n",
    "                        .withColumn(\"valid_records\", lit(valid_records)) \n",
    "                        .withColumn(\"faulty_records\", lit(faulty_records)) \n",
    "                        .withColumn(\"faulty_records_perc\", lit((faulty_records/total_records)*100))\n",
    "                        .withColumn(\"faulty_dqs\", lit(faulty_dqs))\n",
    "            )\n",
    "    else:\n",
    "        schema = StructType([\n",
    "            StructField(\"avg_total_quality_errors\", FloatType()),\n",
    "            StructField(\"total_faulty_columns_perc\", FloatType()),\n",
    "            StructField(\"table_name\", StringType()),\n",
    "            StructField(\"layer\", StringType()),\n",
    "            StructField(\"total_columns_num\", IntegerType()),\n",
    "            StructField(\"total_records\", IntegerType()),\n",
    "            StructField(\"valid_records\", IntegerType()),\n",
    "            StructField(\"faulty_records\", IntegerType()),\n",
    "            StructField(\"faulty_records_perc\", FloatType()),\n",
    "            StructField(\"faulty_dqs\", StringType()),\n",
    "        ])\n",
    "\n",
    "\n",
    "        data = [(0.0, 0.0, table, layer, len(columns), total_records, valid_records, 0, 0.0, \"\")]\n",
    "\n",
    "        df_table_grouped = spark.createDataFrame(data, schema=schema)\n",
    "    \n",
    "    if df_grouped:\n",
    "        df_grouped = df_grouped.unionByName(df_table_grouped)\n",
    "    else:\n",
    "        df_grouped = df_table_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8c3a4-7ede-4e08-800e-1a347073da4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped = df_grouped.withColumn(\"avg_total_quality_errors\", round(\"avg_total_quality_errors\", 2))\n",
    "df_grouped = df_grouped.withColumn(\"total_faulty_columns_perc\", round(\"total_faulty_columns_perc\", 2))\n",
    "df_grouped = df_grouped.withColumn(\"faulty_records_perc\", round(\"faulty_records_perc\", 2))\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925a1e9-1d76-4e45-b5fa-40515efc69c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped.select(\"faulty_dqs\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d878d7-f3eb-402e-a7a0-c765b669e1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_layer = [\n",
    "    [\"enem_microdados\", \"raw\"],\n",
    "    # [\"base_estados_educacao\", \"raw\"],\n",
    "    # [\"dim_candidato\", \"trusted\"],\n",
    "    # [\"dim_escola\", \"trusted\"],\n",
    "    # [\"dim_perfil_gestor\", \"trusted\"],\n",
    "    # [\"dim_projeto_educacao\", \"trusted\"],\n",
    "    # [\"dim_status_redacao\", \"trusted\"],\n",
    "    # [\"dim_tipo_prova\", \"trusted\"],\n",
    "    # [\"fact_candidato\", \"trusted\"],\n",
    "    # [\"enem_results_ibge_view\", \"analytics\"],\n",
    "    # [\"fact_candidato_denormalized\", \"analytics\"]\n",
    "]\n",
    "\n",
    "df_grouped = None\n",
    "for table_info in table_layer:\n",
    "    table = table_info[0]\n",
    "    layer = table_info[1]\n",
    "\n",
    "    df = spark.read.format(\"delta\").load(\n",
    "            (\n",
    "                os.path.join(\n",
    "                    os.getcwd(), \"layers\", layer, \"tables\", f\"{table}_errors\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Defina uma UDF para extrair a coluna 'valor'\n",
    "    def extrair_valor(error_list):\n",
    "        valor = list(list(error_list.items())[0])\n",
    "        return valor\n",
    "\n",
    "    # Registre as UDFs no Spark\n",
    "    schema_valor = ArrayType(StringType())\n",
    "    udf_extrair_valor = udf(extrair_valor, schema_valor)\n",
    "\n",
    "    df_explodido = df.select(explode(\"error_list\").alias(\"explodido\"))\n",
    "    df_explodido = df_explodido.select(explode(\"explodido\").alias(\"explodido\"))\n",
    "    df_resultado = df_explodido.withColumn(\"_explodido\", udf_extrair_valor(\"explodido\"))\n",
    "    df_resultado = df_resultado.withColumn(\"chave\", col(\"_explodido\").getItem(0))\n",
    "    df_resultado = df_resultado.withColumn(\"valor\", col(\"_explodido\").getItem(1))\n",
    "    df_resultado = df_resultado.drop(\"_chave\",\"explodido\",\"_explodido\")\n",
    "\n",
    "    df_resultado = df_resultado.groupBy(\"chave\", \"valor\").agg(count(\"*\").alias(\"total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cbac3-5386-4725-9a53-b056e0a58e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_resultado.show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
